{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SVuJQb0qieHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a5ec61e-a29a-4bae-aa2e-25ef74942193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/Desklop/Uk_Stemmer\n",
            "  Cloning https://github.com/Desklop/Uk_Stemmer to /tmp/pip-req-build-t_mv6rmw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Desklop/Uk_Stemmer /tmp/pip-req-build-t_mv6rmw\n",
            "  Resolved https://github.com/Desklop/Uk_Stemmer to commit a700ae1bd9b69ad84d311d089e8bc95ab7fab44d\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/ukrsyllab.py /content/ukrsyllab.py \n",
        "import pandas, re, random\n",
        "!pip install git+https://github.com/Desklop/Uk_Stemmer\n",
        "from uk_stemmer import UkStemmer #stemmer for ukrainian \n",
        "from ast import literal_eval "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ukrsyllab #syllabificator -- initially for russian(https://github.com/Koziev/rusyllab), slightly modified for ukrainain"
      ],
      "metadata": {
        "id": "Bho8-Luf3ygI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Pseudoword_genUK():\n",
        "\n",
        "  def __init__(self, filename, n_words=300, n_sent=6):\n",
        "    file = open(filename, \"r\", encoding=\"utf8\")\n",
        "    words = file.read().split(\"\\n\")\n",
        "\n",
        "    self.stems = []\n",
        "    for w in words[:50000]: \n",
        "      w = re.sub(\"\\w'\\w\", \"\", w) #remove apostrophe words\n",
        "      w = re.sub(\"\\w-\\w\", \"\", w) #remove hyphenated words\n",
        "      stemmer = UkStemmer()\n",
        "      w = stemmer.stem_word(w) #stem the words from the dataset\n",
        "      if w in words and len(w) > 1 and w.lower() == w: \n",
        "        if w not in self.stems:\n",
        "          self.stems.append(w) #append the stems list with unique lower-case two(or more)-syllable stems \n",
        "\n",
        "    self.n_words = n_words\n",
        "    self.n_sent = n_sent"
      ],
      "metadata": {
        "id": "6smFYxjjgCUt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Wuk7BcufXxqS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2e8487b4-ad55-4b63-bba3-cc8371c745ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  with pandas.option_context('display.max_rows', None,\\n                        'display.max_columns', None,\\n                        'display.precision', 3,\\n                        ): print(df)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "  def probabilities(self): #create a probability matrix for word generation\n",
        "    sound_prob = {} \n",
        "    in_m = {}\n",
        "    for word in self.stems:\n",
        "      i = 0 #index value\n",
        "      morphemes = ukrsyllab.split_words(word.split()) #get syllables -- bigrams used to form a matrix\n",
        "      if len(morphemes) != 1: \n",
        "        if morphemes[i] not in in_m:\n",
        "          in_m[morphemes[i]] = 1/(len(self.stems))\n",
        "        else:\n",
        "          in_m[morphemes[i]] += 1/(len(self.stems))\n",
        "      while i < (len(morphemes) - 1): #while the index is still in the bound of the word\n",
        "        b = morphemes[i]\n",
        "        n = morphemes[i+1]\n",
        "        if n != 0: #in the cases where we have two biagrams following each other, (the initial biagram is not the last biagram of the word)\n",
        "          if b in sound_prob: #if the first biagram is in the database of biagrams\n",
        "            if n in sound_prob: #if the second one is in the database as well\n",
        "              bInd = list(sound_prob).index(n) #find the index of the second biagram\n",
        "              sound_prob[b][bInd] = int(sound_prob[b][bInd]) + 1 #and manipulate the probability value in the row of b that has the index of the following biagram\n",
        "            else:\n",
        "              probs = len(sound_prob) * \"0\" #if the second biagram does not exist in the dataset, we need to add it to the matrix and it needs a value row full of 0's (because its frequency with other sounds is 0)\n",
        "              sound_prob[n] = list(probs) \n",
        "              for x in sound_prob: sound_prob[x].append(0) #every row of the matrix should be appended with 0 because the new sound that is added is also added as a new column that has never occured with the other sounds before\n",
        "              sound_prob[b][-1] = 1 #the last entry of the probability of b should be altered to 1 because the new added sound appeared after that so +1 occurence\n",
        "          else:\n",
        "            probs = len(sound_prob) * \"0\" #if b does not exist in the dataset we should add it\n",
        "            sound_prob[b] = list(probs)\n",
        "            for x in sound_prob: sound_prob[x].append(0)\n",
        "            if n in sound_prob:\n",
        "              bInd = list(sound_prob).index(n)\n",
        "              sound_prob[b][bInd] = int(sound_prob[b][bInd]) + 1\n",
        "            else:\n",
        "              sound_prob[n] = list(probs)\n",
        "              sound_prob[n].append(0)\n",
        "              for x in sound_prob: sound_prob[x].append(0)\n",
        "              sound_prob[b][-1] = 1\n",
        "        i += 1\n",
        "    df = pandas.DataFrame.from_dict(sound_prob, orient=\"index\", columns=sound_prob.keys()) #display the matrix\n",
        "    df = df.applymap(int) #make every entry of the matrix an integer\n",
        "    df = df/len(sound_prob.keys()) #calculate the probabilities each occurence should be divided with all occurences\n",
        "    return df, sound_prob, in_m\n",
        "\n",
        "  \"\"\"  with pandas.option_context('display.max_rows', None,\n",
        "                          'display.max_columns', None,\n",
        "                          'display.precision', 3,\n",
        "                          ): print(df)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  def suffixation (self, p_list): #ascribe suffixes to the pseudostems to make them morphologically recognizable \n",
        "    global p_words\n",
        "    p_words = []\n",
        "    suffixes = ['о-таки', 'о-то','но','цька','ий','ик','ник','івник','льник','иво','аль','ень','ець','ість','тель','иця','иня','ння','іння','ання','яння','ення','иння','еня','ечок','ечка','ечко','ичок','ичка','енко','енько','исько','ище','івка','овка','ок','ир','ист','изм','ір','іст','ізм','яти','ати','іти']\n",
        "    ending = ['б', 'в', 'г', 'ґ', 'д', 'ж', 'з', 'к', 'л', 'м', 'н', 'п', 'р', 'с', 'т', 'ф', 'х', 'ц', 'ч', 'ш', 'щ']\n",
        "    for instance in p_list: \n",
        "        if (list(filter(instance.endswith, ending))): #if the ending of the pseudostem matches any ending of the 'ending' list, it gets a random suffix from the 'suffixes' list     \n",
        "          norm = instance + random.choice(suffixes)\n",
        "          if norm not in p_words: #pseudowords list gets appended with the unique pseudoword\n",
        "            p_words.append(norm)\n",
        "    return p_words"
      ],
      "metadata": {
        "id": "Myn9kI_LLtMe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  def clean (self, ps_list): #wipe out repeating syllables, vowels or consonants (normalize them to make more natural for ukrainian)\n",
        "    ps_list = re.sub(r'(.+?)\\1+', r'\\1', str(ps_list))   \n",
        "    ps_list = literal_eval(str(ps_list)) #as the list needs to be converted into a string in the previous line, it gets converted back into a list\n",
        "    return ps_list"
      ],
      "metadata": {
        "id": "BRcC-xhfydxw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  def categorize (self, dataset): #classify pseudowords by their functions within the sentence, based on their morphological endings\n",
        "    uk_pos = {\"SUBJECT\": [],\n",
        "              \"PREDICATE\": [],\n",
        "              \"ATTRIBUTE\":[],\n",
        "              \"OBJECT\": [],\n",
        "              \"ADVERBIAL MODIFIER\": []}\n",
        "    \n",
        "    #suffix lists, by which functions within the sentence are implied\n",
        "    subj_suff = ['ик','ник','івник','льник','иво','аль','ень','ець','ість','тель','иця','иня','ння','іння','ання','яння','ення','иння','еня','ечок','ечка','ечко','ичок','ичка','енко','исько','ище','івка','овка','ок','ир','ист','изм','ір','іст','ізм']\n",
        "    pred_suff = ['ти']\n",
        "    attr_suff = ['ий']\n",
        "    mod_suff = ['-таки', '-то', 'но', 'ацька' ]\n",
        "\n",
        "    pr = []\n",
        "    attr = []\n",
        "    obj = []\n",
        "\n",
        "    for word in dataset: #iterate though the dataset\n",
        "        if any(word.endswith(s) for s in subj_suff): #iterate through the suffix list\n",
        "              uk_pos[\"SUBJECT\"].append(word) #if there's a match, add pseudowords in the nominative case to the dictionary -- subjects\n",
        "              obj.append(word) #if there's a match, add pseudowords in the nominative case to the specific list to later be inflected -- objects \n",
        "        elif any(word.endswith(mod) for mod in mod_suff): \n",
        "              uk_pos[\"ADVERBIAL MODIFIER\"].append(word) #if there's a match, add pseudowords in the nominative case to the dictionary -- adverbial modifiers\n",
        "        elif any(word.endswith(p) for p in pred_suff): \n",
        "              pr.append(word) #if there's a match, add pseudowords in the nominative case to the specific list to later be inflected -- predicates\n",
        "        elif any(word.endswith(at) for at in attr_suff):\n",
        "              attr.append(word) #if there's a match, add pseudowords in the nominative case to the specific list to later be inflected -- attributes\n",
        "\n",
        "    def infl_dict(infl_suff, inf_suff, lst, dict_key): #function that inflects predicates and attributes (for accusative), and adds them to the dictionary\n",
        "      for st in lst: #iterate though the list\n",
        "        for af in infl_suff: #iterate through the nominative suffix list\n",
        "          ret = re.sub(inf_suff, af, st) #substitute the ending in the nominative case with the ending in the accusative\n",
        "          uk_pos[dict_key].append(ret) \n",
        "\n",
        "    infl_dict(infl_suff=['в','ла', 'ло'], inf_suff = 'ти', lst=pr, dict_key=\"PREDICATE\")\n",
        "    infl_dict(infl_suff=['ої','ого'], inf_suff = 'ий', lst=attr, dict_key=\"ATTRIBUTE\")\n",
        "\n",
        "    def infl_dict_obj (gendered_suffix, inflection): #function that inflects objects (for accusative) based on the inflection rules for gendered nouns (more condition neccessary for objects)\n",
        "        for ob in obj: #iterate though the list\n",
        "          if any(ob.endswith(s) for s in gendered_suffix): #iterate through the gendered nominative suffix list\n",
        "            if ob[-1] in ['м', 'р', 'к', 'т']: #nouns ending with suffixes from the list, and with these letters specifically, get another letter added\n",
        "              ob = ob + inflection\n",
        "            elif ob[-1] in ['ь']: #other nouns that match change the last letter \n",
        "              ob = ob[:-1] + inflection\n",
        "            elif ob[-1] in ['я']:\n",
        "              ob = ob[:-1] + inflection\n",
        "            elif ob[-1] in ['а']:\n",
        "              ob = ob[:-1] + inflection\n",
        "            elif ob[-1] in ['о', 'е']:\n",
        "              ob = ob[:-1] + inflection\n",
        "            uk_pos[\"OBJECT\"].append(ob)\n",
        "\n",
        "    infl_dict_obj(gendered_suffix=['ик','ник','івник','льник','ок','ир','ист','изм', 'ір', 'іст', 'ізм'], inflection = 'а') #masculine nouns that end in a hard consonant\n",
        "    infl_dict_obj(gendered_suffix= ['аль', 'ень' 'тель', 'ець'], inflection = 'я') #masculine nouns that end in a soft consonant\n",
        "    infl_dict_obj(gendered_suffix=['иця','иня','ння','іння','ання','яння','иння'], inflection = 'і') #feminine nouns that end in a soft consonant\n",
        "    infl_dict_obj(gendered_suffix=['ичка', 'івка', 'овка'], inflection = 'и') #feminine nouns that end in a hard consonant\n",
        "    infl_dict_obj(gendered_suffix=['иво', 'ечко', 'енко', 'исько','ище'], inflection = 'а') #neutral nouns\n",
        "    return uk_pos"
      ],
      "metadata": {
        "id": "hjn9O_QTOr5s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  def sent_generator(self, dic): #generate sentences with the dictionary values, based on the dictionary keys\n",
        "    subj = random.choice(list(dic[\"SUBJECT\"])) #pick a random value\n",
        "    pred = random.choice(list(dic[\"PREDICATE\"]))\n",
        "    attr = random.choice(list(dic[\"ATTRIBUTE\"]))\n",
        "    obje = random.choice(list(dic[\"OBJECT\"]))\n",
        "    adve = random.choice(list(dic[\"ADVERBIAL MODIFIER\"]))\n",
        "\n",
        "    if re.search('[ое]$', subj): #coordinate subject and predicate by gender\n",
        "        while not re.search('ло$', pred): #if a chosen subject matches a certain pattern, a predicate must match a specific pattern too -- if it doesn't, a different value is being chosen until the conditions are satisfied.\n",
        "          pred = random.choice(list(dic[\"PREDICATE\"]))     \n",
        "    elif re.search('[аяь]$', subj):\n",
        "        while not re.search('ла$', pred):\n",
        "          pred = random.choice(list(dic[\"PREDICATE\"]))\n",
        "    elif re.search('[кмстр]$', subj):\n",
        "        while not re.search('в$', pred):\n",
        "          pred = random.choice(list(dic[\"PREDICATE\"]))\n",
        "      \n",
        "    if re.search('[иі]$', obje): #coordinate attribute and object by gender\n",
        "        while not re.search('ої$', attr): #if a chosen object matches a certain pattern, an attribute must match a specific pattern too -- if it doesn't, a different value is being chosen until the conditions are satisfied.\n",
        "          attr = random.choice(list(dic[\"ATTRIBUTE\"]))\n",
        "    elif re.search('[ая]$', obje): \n",
        "        while not re.search('ого$', attr):\n",
        "          attr = random.choice(list(dic[\"ATTRIBUTE\"]))     \n",
        "\n",
        "    sent_str = random.choice([\"sent_str1\", \"sent_str2\",  \"sent_str3\"]) #create random sentences using a randomly chosen structure, natural to the ukrainian syntax\n",
        "    if sent_str == \"sent_str1\":\n",
        "      sent = subj + \" \" + pred + \" \" + attr + \" \" + obje + \" \" + adve + \".\"\n",
        "    elif sent_str == \"sent_str2\":\n",
        "      sent = adve + \" \" + subj + \" \" + pred + \" \" + attr + \" \" + obje + \".\"\n",
        "    else: \n",
        "      sent = attr + \" \" + obje + \" \" + pred + \" \" + subj + \" \" + adve + \".\"\n",
        "    sent = sent.capitalize()\n",
        "    return sent"
      ],
      "metadata": {
        "id": "9fD3gzCZAMo2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  def run(self):\n",
        "      df_prob, dict_prob, in_morph = self.probabilities()\n",
        "      pre_p_words = [] \n",
        "      j=0\n",
        "      while j < self.n_words: #create pseudostems\n",
        "        pword = []\n",
        "        i = 0\n",
        "        plen = 4 \n",
        "        while i < plen:\n",
        "          if pword == []:\n",
        "            sound = random.choices(list(in_morph.keys()), in_morph.values())[0] #start with a morpheme that is most likely to be in the beginning of the word \n",
        "            i += len(sound)\n",
        "          else:\n",
        "            sound = random.choices(list(dict_prob.keys()), list(df_prob[pword[-1]]))[0] #proceed with other morphemes until the length requirement is satisfied\n",
        "            i += len(sound)\n",
        "          pword.append(sound)\n",
        "          pword = \"\".join(pword)\n",
        "        pre_p_words.append(pword)\n",
        "        p_words = self.suffixation(pre_p_words) #run function to suffixate the stems\n",
        "        p_words = self.clean(p_words) #run function to normalize the pseudowords \n",
        "        j += 1\n",
        "\n",
        "      word_categories = self.categorize(p_words) #run function to sort the parts of the sentences into the dictionary\n",
        "      p_sentences = []\n",
        "      for i in range(self.n_sent):\n",
        "        p_sentences.append(self.sent_generator(word_categories)) #run function to create sentences\n",
        "      print(p_sentences)\n",
        "      return p_sentences"
      ],
      "metadata": {
        "id": "n44XyVxXBSz9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = Pseudoword_genUK(\"uk_UA.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "y2aX5GjQ3Oj2",
        "outputId": "7a3246b4-47a7-43e9-cbac-6a2b0e9bf0e8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-79901fcff0af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPseudoword_genUK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uk_UA.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-6bf681a6a179>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, n_words, n_sent)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Pseudoword_genUK' object has no attribute 'run'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "KARRgDrJ41tG",
        "outputId": "1b687f3a-334c-4c28-ddac-dd9385afec7e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-8b1a508a8b9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Pseudoword_genUK' object has no attribute 'run'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}